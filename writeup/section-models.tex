\section{Formal Framework}
\label{sec:models}

In this section, we describe formal details of the ``rational speech act'' model. As described above, this model has been presented previously \cite{frank2012,goodman2013}. Our goal here is to provide a more comprehensive formal presentation. This presentation allows us to highlight choice-points in the model that we test below; in addition, it illuminates parallels and differences with other formal models in this space (specific comparisons are given in Appendix \ref{app:equivalences}).

% We introduce a notation for signaling games and other recursive reasoning problems \cite{golland2010,franke2012,frank2012,goodman2013}. This notation system allows us to define a set of recursive models; we show that a set of recent systems for pragmatic reasoning can be written within this system. As a consequence, they are equivalent to one another modulo three design decisions: 


\subsection{Preliminaries}

A reference game under our definition is a game in which a speaker $S$ and listener $L$ collaborate in a context $C$ to identify a particular object in the context, known as the speaker's intended referent $r_S \in C$. The game has two parts. First the speaker chooses an utterance $w$ based on vocabulary $V$; this process can be as simple as selection from a list or as complex as generation from a grammar. Next the listener guesses a referent $r_L \in C$ after hearing $w$. The game is won if $r_S=r_L$.

There is a ``contextual salience'' distribution $\sigma$ over the objects ${o_1 ... o_n} \in C$, which is mutually known to both speaker and hearer. This distribution picks out those objects in the context which are more or less likely to be talked about, either because of their intrinsic perceptual or conceptual noteworthiness, or because of some prior history between speaker and listener (e.g. one object having been talked about previously). 

There is also a mutually-known semantics for the vocabulary of possible words (messages) $w$ that a speaker can send.  Each message can be described as a Kronecker $\delta$ function that applies to referents and returns 1 if the message is true of that referent, 0 otherwise. 

\subsection{The model}

We define the RSA model in terms of two agents, a listener $L$ and a speaker $S$. Their goals are to reason about one another so that they are able to transmit information efficiently. $L$ reasons abut what word $S$ would have said to describe a particular referent; $S$ reasons about what interpretation $L$ would give to a particular message. Each of these agents uses Bayesian inference to reason about the other's likely actions. 

So we can define:

\begin{equation}
  \begin{array}{r@{}l}
    \label{eq:agents}
    P_L(r_S | w, C) & \propto P_S (w | r_S, C) P(r_S)\\
    P_S(w | r, C) & \propto P_L (r_S | w, C)  
  \end{array}
\end{equation}

\noindent where $P(r_S)$ is a prior distribution over referents, which we discuss extensively below. In principle, we could also add a prior on words, $P(w)$, but for simplicity here we assume that $P(w) \propto 1$ and do not discuss it further. 

The two agents in Equation \ref{eq:agents} are defined recursively and so individual probabilities are undefined unless the recursion ends. Thus, we define a further agent, $L_0$, the ``literal listener,'' who grounds the recursion:

\begin{equation}
P_{L_0} (r_S | w, C) \propto \begin{cases} \frac{1}{\sum_{r \in C} \delta_w(r)} &\mbox{if } \delta_w(r_s) = 1 \\ 
0 & \mbox{otherwise.} \end{cases} 
\end{equation}

\noindent In other words, agent $L_0$ simply chooses uniformly between available referents that are consistent with $w$. 

With these agents defined, we can then imagine variations on this model where $L$ reasons about $S$, who in turn reasons about $L_0$. 

See Appendix \ref{app:alternative}

\subsection{Choice-points}

\begin{enumerate}
\item The depth of the recursion between speaker and listener,
\item Whether the recursion starts with speaker or listener,
\item The decision rule chosen by speaker and listener (soft vs. hard maximization), and 
\item Whether the model includes a term indicating the prior probability of a referent being talked about.
\end{enumerate}

\noindent We show equivalences of various special cases of this notation to the systems described in these prior reports.

\subsection{An example}

% \begin{figure}[t]
%   \begin{center} 
%     \includegraphics[width=4.5in]{figures/bugs.jpg} 
%     \caption{\label{fig:ex} An example stimulus for our 3 objects, 3 features experiment. Inferring that ``tail'' referred to C is a depth-0 computation, inferring that ``feet'' referred to A is a depth-1 computation, and inferring that ``horns'' referred to B is a depth-2 computation.} 
%   \end{center} 
% \end{figure}

We work through an example computation on the stimulus shown in Figure \ref{fig:ex}, using the J\"ager \cite{jaegerinpress} version of the model described above (for simplicity and finite convergence): $L^*(S^*(L^*(S(C^T))))$. The object by feature matrix $C$ has as its columns the messages ``tail,'' ``horns,'' and ``feet'' and the objects A, B, and C as its rows.:

\begin{equation}
C= \left(
    \begin{array}{ccc}
      0 & 0 & 1 \\
      0 & 1 & 1\\
      1 & 1 & 0 
    \end{array} 
  \right)
\end{equation}

The corresponding speaker matrix $S(C^T)$ is inverted, with messages as rows and objects as columns: 

\begin{equation}
E = \left(
    \begin{array}{ccc}
      0 & 0 & 1 \\
      0 & .5 & .5\\
      .5 & .5 & 0 
    \end{array} 
  \right)
\end{equation}

It is already clear that ``tail'' is true only of object C, thus the interpretation of ``tail'' reaches depth 0. Now we perform a set of renormalizations to lead to the next level of recursion:

\begin{equation}
L^*(S(C^T)) = \left(
    \begin{array}{ccc}
      0 & 0 & 1 \\
      0 & .5 & .5\\
      1 & 0 & 0 
    \end{array} 
  \right)
\end{equation}

Now, at depth 1, the message ``feet'' is found to refer to object A. A final round of recursion leads to depth 2, where ``horns'' is found to refer to object B:

\begin{equation}
L^*(S^*(L^*(S(C^T)))) = \left(
    \begin{array}{ccc}
      0 & 0 & 1 \\
      0 & 1 & 0\\
      1 & 0 & 0 
    \end{array} 
  \right)
\end{equation}

We now say that the model has ``converged''---that is, further iteration will not lead to changes in state.

\subsection{Status of the model}

%%% Local Variables: 
%%% TeX-master: "pragmods"
%%% End:
